{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1e8a625e",
   "metadata": {},
   "source": [
    "# 01 Data Collection — Enhanced Pipeline (v0.2)\n",
    "\n",
    "## Purpose\n",
    "Scrape LinkedIn job listings, visit each job page to extract the full description,\n",
    "and produce a **structured, ML-ready dataset** with normalized fields.\n",
    "\n",
    "## Pipeline Phases\n",
    "1. **Phase 1 — Listing Scrape**: Fetch job cards (title, company, location, link)\n",
    "2. **Phase 2 — Detail Extraction**: Visit each link → pull full job description\n",
    "3. **Phase 3 — NLP Parsing**: Title normalization, skills extraction, seniority inference, etc.\n",
    "4. **Phase 4 — Quality & Output**: Validate schema → save versioned Parquet + CSV\n",
    "\n",
    "## IO\n",
    "- **Input**: Search parameters (keywords, location, limit)\n",
    "- **Output**:\n",
    "  - `data/processed/jobs_latest.parquet` — structured, ML-ready dataset\n",
    "  - `data/raw/raw_listings_*.json` — raw listing data (for reprocessing)\n",
    "  - `outputs/tables/scrape_report_*.json` — quality report\n",
    "  - `outputs/logs/scrape_*.log` — full run log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "42e52965",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Schema version: 0.2\n",
      "Dataset version: v0.2\n",
      "Expected fields: 25\n"
     ]
    }
   ],
   "source": [
    "import sys, os\n",
    "\n",
    "# Ensure the project root is on the path so `src.*` imports work\n",
    "project_root = os.path.abspath(os.path.join(os.getcwd(), \"..\"))\n",
    "if project_root not in sys.path:\n",
    "    sys.path.insert(0, project_root)\n",
    "\n",
    "import pandas as pd\n",
    "import json\n",
    "import logging\n",
    "from datetime import datetime\n",
    "\n",
    "# Project modules\n",
    "from src.scraping.linkedin_scraper import LinkedInScraper\n",
    "from src.preprocessing.job_parser import parse_job\n",
    "from src.preprocessing.vocabularies import SCHEMA_VERSION, DATASET_VERSION, SCHEMA_FIELDS\n",
    "from src.pipeline import (\n",
    "    setup_logging,\n",
    "    validate_schema,\n",
    "    save_dataset,\n",
    "    save_raw_listings,\n",
    "    save_scrape_report,\n",
    "    run_pipeline,\n",
    ")\n",
    "\n",
    "# Ensure output directories exist\n",
    "for d in [\"../data/raw\", \"../data/processed\", \"../data/interim\",\n",
    "          \"../outputs/logs\", \"../outputs/tables\", \"../outputs/figures\"]:\n",
    "    os.makedirs(d, exist_ok=True)\n",
    "\n",
    "print(f\"Schema version: {SCHEMA_VERSION}\")\n",
    "print(f\"Dataset version: {DATASET_VERSION}\")\n",
    "print(f\"Expected fields: {len(SCHEMA_FIELDS)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "361c396d",
   "metadata": {},
   "source": [
    "## Configuration\n",
    "Set search parameters here. Keep `limit` at 20–30 during development."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f4182f6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Search: 'Data Scientist' | Location: 'worldwide' | Limit: 25\n"
     ]
    }
   ],
   "source": [
    "# ── Search parameters ──\n",
    "KEYWORDS = \"Data Scientist\"\n",
    "LOCATION = \"\"          # Empty = worldwide\n",
    "LIMIT = 25             # Keep at 20-30 for dev; scale later\n",
    "\n",
    "# ── Output paths (relative to notebook) ──\n",
    "RAW_OUTPUT_DIR = \"../data/raw\"\n",
    "PROCESSED_OUTPUT_DIR = \"../data/processed\"\n",
    "\n",
    "print(f\"Search: '{KEYWORDS}' | Location: '{LOCATION or 'worldwide'}' | Limit: {LIMIT}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8d91ad6",
   "metadata": {},
   "source": [
    "## Run Pipeline\n",
    "Execute the full scraping + enrichment + parsing pipeline.\n",
    "\n",
    "This will:\n",
    "1. Scrape listing cards from LinkedIn (public guest API)\n",
    "2. Visit each job URL and extract the full description\n",
    "3. Parse each description into structured fields (title, skills, seniority, etc.)\n",
    "4. Apply quality gates (discard records with no description or no skills)\n",
    "5. Save versioned Parquet + CSV outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "91597213",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-02-10 20:53:10,287 | INFO    | src.pipeline | ============================================================\n",
      "2026-02-10 20:53:10,288 | INFO    | src.pipeline | TOP APPLICANT — Data Collection Pipeline\n",
      "2026-02-10 20:53:10,289 | INFO    | src.pipeline | Schema version: 0.2\n",
      "2026-02-10 20:53:10,292 | INFO    | src.pipeline | Dataset version: v0.2\n",
      "2026-02-10 20:53:10,293 | INFO    | src.pipeline | Search: 'Data Scientist' | Location: '' | Limit: 25\n",
      "2026-02-10 20:53:10,295 | INFO    | src.pipeline | ============================================================\n",
      "2026-02-10 20:53:10,296 | INFO    | src.scraping.linkedin_scraper | Phase 1: Scraping up to 25 listings for 'Data Scientist' in ''\n",
      "2026-02-10 20:53:12,025 | INFO    | src.scraping.linkedin_scraper |   [1] Data Scientist Intern @ Tinder\n",
      "2026-02-10 20:53:12,026 | INFO    | src.scraping.linkedin_scraper |   [2] Data Scientist I, Integrity @ Tinder\n",
      "2026-02-10 20:53:12,027 | INFO    | src.scraping.linkedin_scraper |   [3] Data Scientist (L5) , Performance Marketing @ Netflix\n",
      "2026-02-10 20:53:12,028 | INFO    | src.scraping.linkedin_scraper |   [4] Head of Artificial Intelligence and Data Science @ Addison Group\n",
      "2026-02-10 20:53:12,029 | INFO    | src.scraping.linkedin_scraper |   [5] Data Scientist (L4) -  Marketing and Experience DSE @ Netflix\n",
      "2026-02-10 20:53:12,031 | INFO    | src.scraping.linkedin_scraper |   [6] Data Scientist Analyst @ MedReview Inc.\n",
      "2026-02-10 20:53:12,032 | INFO    | src.scraping.linkedin_scraper |   [7] Data Scientist @ Roku\n",
      "2026-02-10 20:53:12,033 | INFO    | src.scraping.linkedin_scraper |   [8] Data Scientist I, Integrity @ Tinder\n",
      "2026-02-10 20:53:12,034 | INFO    | src.scraping.linkedin_scraper |   [9] Data Scientist I, Integrity @ Tinder\n",
      "2026-02-10 20:53:12,035 | INFO    | src.scraping.linkedin_scraper |   [10] Machine Learning Engineer Intern @ Tinder\n",
      "2026-02-10 20:53:16,484 | INFO    | src.scraping.linkedin_scraper |   [11] Data Scientist 5 - Ads Identity @ Netflix\n",
      "2026-02-10 20:53:16,484 | INFO    | src.scraping.linkedin_scraper |   [12] Data Scientist 4/5 - Identity DSE @ Netflix\n",
      "2026-02-10 20:53:16,485 | INFO    | src.scraping.linkedin_scraper |   [13] Data Scientist, Product Analytics @ Meta\n",
      "2026-02-10 20:53:16,486 | INFO    | src.scraping.linkedin_scraper |   [14] Data Scientist - Data-Driven Global Investment Firm @ Andiamo\n",
      "2026-02-10 20:53:16,487 | INFO    | src.scraping.linkedin_scraper |   [15] Data Scientist - Campus Full-Time @ Two Sigma\n",
      "2026-02-10 20:53:16,489 | INFO    | src.scraping.linkedin_scraper |   [16] Data Scientist - Decisions, Lyft Urban Solutions @ Lyft\n",
      "2026-02-10 20:53:16,490 | INFO    | src.scraping.linkedin_scraper |   [17] Staff Data Scientist @ LinkedIn\n",
      "2026-02-10 20:53:16,492 | INFO    | src.scraping.linkedin_scraper |   [18] Data Scientist @ Two Sigma\n",
      "2026-02-10 20:53:16,493 | INFO    | src.scraping.linkedin_scraper |   [19] Data Scientist @ StreetID\n",
      "2026-02-10 20:53:16,495 | INFO    | src.scraping.linkedin_scraper |   [20] Principal Research Scientist @ Next Phase Recruitment\n",
      "2026-02-10 20:53:21,213 | INFO    | src.scraping.linkedin_scraper |   [21] Data Scientist, Lifecycle Marketing @ Wealthfront\n",
      "2026-02-10 20:53:21,214 | INFO    | src.scraping.linkedin_scraper |   [22] Data Scientist @ Kinect\n",
      "2026-02-10 20:53:21,214 | INFO    | src.scraping.linkedin_scraper |   [23] Data Scientist, Product Analytics @ Meta\n",
      "2026-02-10 20:53:21,217 | INFO    | src.scraping.linkedin_scraper |   [24] Data / ML Ops Engineer @ PBG\n",
      "2026-02-10 20:53:21,218 | INFO    | src.scraping.linkedin_scraper |   [25] Data Scientist @ Juul Labs\n",
      "2026-02-10 20:53:25,080 | INFO    | src.scraping.linkedin_scraper | Phase 1 complete: 25 listings scraped.\n",
      "2026-02-10 20:53:25,084 | INFO    | src.pipeline | Saved raw listings: ../data/raw\\raw_listings_20260210_205325.json\n",
      "2026-02-10 20:53:25,085 | INFO    | src.scraping.linkedin_scraper | Phase 2: Enriching 25 listings with full descriptions\n",
      "2026-02-10 20:53:25,086 | INFO    | src.scraping.linkedin_scraper |   [1/25] Fetching: https://www.linkedin.com/jobs/view/data-scientist-intern-at-tinder-4323761210\n",
      "2026-02-10 20:53:29,498 | INFO    | src.scraping.linkedin_scraper |     ✓ Parsed: Data Scientist | Seniority: Senior | Skills: 4 required\n",
      "2026-02-10 20:53:29,499 | INFO    | src.scraping.linkedin_scraper |   [2/25] Fetching: https://www.linkedin.com/jobs/view/data-scientist-i-integrity-at-tinder-4367025321\n",
      "2026-02-10 20:53:35,300 | INFO    | src.scraping.linkedin_scraper |     ✓ Parsed: Data Scientist | Seniority: Senior | Skills: 6 required\n",
      "2026-02-10 20:53:35,301 | INFO    | src.scraping.linkedin_scraper |   [3/25] Fetching: https://www.linkedin.com/jobs/view/data-scientist-l5-performance-marketing-at-netflix-4313094719\n",
      "2026-02-10 20:53:39,477 | INFO    | src.scraping.linkedin_scraper |     ✓ Parsed: Data Scientist | Seniority: Senior | Skills: 6 required\n",
      "2026-02-10 20:53:39,478 | INFO    | src.scraping.linkedin_scraper |   [4/25] Fetching: https://www.linkedin.com/jobs/view/head-of-artificial-intelligence-and-data-science-at-addison-group-4368254991\n",
      "2026-02-10 20:53:44,224 | INFO    | src.scraping.linkedin_scraper |     ✓ Parsed: Head of Artificial Intelligence and Data Science | Seniority: Staff/Lead | Skills: 2 required\n",
      "2026-02-10 20:53:44,225 | INFO    | src.scraping.linkedin_scraper |   [5/25] Fetching: https://www.linkedin.com/jobs/view/data-scientist-l4-marketing-and-experience-dse-at-netflix-4341990612\n",
      "2026-02-10 20:53:47,888 | INFO    | src.scraping.linkedin_scraper |     ✓ Parsed: Data Scientist | Seniority: Senior | Skills: 4 required\n",
      "2026-02-10 20:53:47,889 | INFO    | src.scraping.linkedin_scraper |   [6/25] Fetching: https://www.linkedin.com/jobs/view/data-scientist-analyst-at-medreview-inc-4361492622\n",
      "2026-02-10 20:53:50,786 | INFO    | src.scraping.linkedin_scraper |     ✓ Parsed: Data Scientist | Seniority: Senior | Skills: 8 required\n",
      "2026-02-10 20:53:50,788 | INFO    | src.scraping.linkedin_scraper |   [7/25] Fetching: https://www.linkedin.com/jobs/view/data-scientist-at-roku-4359751153\n",
      "2026-02-10 20:53:57,357 | INFO    | src.scraping.linkedin_scraper |     ✓ Parsed: Data Scientist | Seniority: Senior | Skills: 8 required\n",
      "2026-02-10 20:53:57,358 | INFO    | src.scraping.linkedin_scraper |   [8/25] Fetching: https://www.linkedin.com/jobs/view/data-scientist-i-integrity-at-tinder-4367007948\n",
      "2026-02-10 20:54:01,503 | INFO    | src.scraping.linkedin_scraper |     ✓ Parsed: Data Scientist | Seniority: Senior | Skills: 6 required\n",
      "2026-02-10 20:54:01,504 | INFO    | src.scraping.linkedin_scraper |   [9/25] Fetching: https://www.linkedin.com/jobs/view/data-scientist-i-integrity-at-tinder-4367011795\n",
      "2026-02-10 20:54:06,417 | INFO    | src.scraping.linkedin_scraper |     ✓ Parsed: Data Scientist | Seniority: Senior | Skills: 6 required\n",
      "2026-02-10 20:54:06,418 | INFO    | src.scraping.linkedin_scraper |   [10/25] Fetching: https://www.linkedin.com/jobs/view/machine-learning-engineer-intern-at-tinder-4323750983\n",
      "2026-02-10 20:54:11,791 | INFO    | src.scraping.linkedin_scraper |     ✓ Parsed: Machine Learning Engineer | Seniority: Senior | Skills: 3 required\n",
      "2026-02-10 20:54:11,793 | INFO    | src.scraping.linkedin_scraper |   [11/25] Fetching: https://www.linkedin.com/jobs/view/data-scientist-5-ads-identity-at-netflix-4356549998\n",
      "2026-02-10 20:54:14,889 | INFO    | src.scraping.linkedin_scraper |     ✓ Parsed: Data Scientist | Seniority: Senior | Skills: 5 required\n",
      "2026-02-10 20:54:14,890 | INFO    | src.scraping.linkedin_scraper |   [12/25] Fetching: https://www.linkedin.com/jobs/view/data-scientist-4-5-identity-dse-at-netflix-4348419825\n",
      "2026-02-10 20:54:19,525 | INFO    | src.scraping.linkedin_scraper |     ✓ Parsed: Data Scientist | Seniority: Senior | Skills: 7 required\n",
      "2026-02-10 20:54:19,525 | INFO    | src.scraping.linkedin_scraper |   [13/25] Fetching: https://www.linkedin.com/jobs/view/data-scientist-product-analytics-at-meta-4120823523\n",
      "2026-02-10 20:54:24,446 | INFO    | src.scraping.linkedin_scraper |     ✓ Parsed: Data Scientist | Seniority: Senior | Skills: 8 required\n",
      "2026-02-10 20:54:24,447 | INFO    | src.scraping.linkedin_scraper |   [14/25] Fetching: https://www.linkedin.com/jobs/view/data-scientist-data-driven-global-investment-firm-at-andiamo-4306618897\n",
      "2026-02-10 20:54:28,797 | INFO    | src.scraping.linkedin_scraper |     ✓ Parsed: Data Scientist | Seniority: Senior | Skills: 2 required\n",
      "2026-02-10 20:54:28,798 | INFO    | src.scraping.linkedin_scraper |   [15/25] Fetching: https://www.linkedin.com/jobs/view/data-scientist-campus-full-time-at-two-sigma-4351354278\n",
      "2026-02-10 20:54:32,343 | INFO    | src.scraping.linkedin_scraper |     ✓ Parsed: Data Scientist | Seniority: Senior | Skills: 2 required\n",
      "2026-02-10 20:54:32,344 | INFO    | src.scraping.linkedin_scraper |   [16/25] Fetching: https://www.linkedin.com/jobs/view/data-scientist-decisions-lyft-urban-solutions-at-lyft-4369992190\n",
      "2026-02-10 20:54:35,141 | INFO    | src.scraping.linkedin_scraper |     ✓ Parsed: Data Scientist | Seniority: Senior | Skills: 2 required\n",
      "2026-02-10 20:54:35,141 | INFO    | src.scraping.linkedin_scraper |   [17/25] Fetching: https://www.linkedin.com/jobs/view/staff-data-scientist-at-linkedin-4365223573\n",
      "2026-02-10 20:54:37,960 | INFO    | src.scraping.linkedin_scraper |     ✓ Parsed: Data Scientist | Seniority: Staff/Lead | Skills: 11 required\n",
      "2026-02-10 20:54:37,960 | INFO    | src.scraping.linkedin_scraper |   [18/25] Fetching: https://www.linkedin.com/jobs/view/data-scientist-at-two-sigma-4209153448\n",
      "2026-02-10 20:54:42,694 | INFO    | src.scraping.linkedin_scraper |     ✓ Parsed: Data Scientist | Seniority: Senior | Skills: 2 required\n",
      "2026-02-10 20:54:42,694 | INFO    | src.scraping.linkedin_scraper |   [19/25] Fetching: https://www.linkedin.com/jobs/view/data-scientist-at-streetid-3819704675\n",
      "2026-02-10 20:54:46,901 | INFO    | src.scraping.linkedin_scraper |     ✓ Parsed: Data Scientist | Seniority: Senior | Skills: 3 required\n",
      "2026-02-10 20:54:46,902 | INFO    | src.scraping.linkedin_scraper |   [20/25] Fetching: https://www.linkedin.com/jobs/view/principal-research-scientist-at-next-phase-recruitment-4357715025\n",
      "2026-02-10 20:54:49,781 | INFO    | src.scraping.linkedin_scraper |     ✓ Parsed: Research Scientist | Seniority: Staff/Lead | Skills: 4 required\n",
      "2026-02-10 20:54:49,782 | INFO    | src.scraping.linkedin_scraper |   [21/25] Fetching: https://www.linkedin.com/jobs/view/data-scientist-lifecycle-marketing-at-wealthfront-4330764625\n",
      "2026-02-10 20:54:54,508 | INFO    | src.scraping.linkedin_scraper |     ✓ Parsed: Data Scientist | Seniority: Senior | Skills: 5 required\n",
      "2026-02-10 20:54:54,509 | INFO    | src.scraping.linkedin_scraper |   [22/25] Fetching: https://www.linkedin.com/jobs/view/data-scientist-at-kinect-4370029551\n",
      "2026-02-10 20:54:58,986 | INFO    | src.scraping.linkedin_scraper |     ✓ Parsed: Data Scientist | Seniority: Senior | Skills: 6 required\n",
      "2026-02-10 20:54:58,986 | INFO    | src.scraping.linkedin_scraper |   [23/25] Fetching: https://www.linkedin.com/jobs/view/data-scientist-product-analytics-at-meta-4120391416\n",
      "2026-02-10 20:55:01,742 | INFO    | src.scraping.linkedin_scraper |     ✓ Parsed: Data Scientist | Seniority: Senior | Skills: 8 required\n",
      "2026-02-10 20:55:01,743 | INFO    | src.scraping.linkedin_scraper |   [24/25] Fetching: https://www.linkedin.com/jobs/view/data-ml-ops-engineer-at-pbg-4367189438\n",
      "2026-02-10 20:55:04,162 | INFO    | src.scraping.linkedin_scraper |     ✓ Parsed: Data / ML Ops Engineer | Seniority: Senior | Skills: 11 required\n",
      "2026-02-10 20:55:04,163 | INFO    | src.scraping.linkedin_scraper |   [25/25] Fetching: https://www.linkedin.com/jobs/view/data-scientist-at-juul-labs-4344769602\n",
      "2026-02-10 20:55:08,979 | INFO    | src.scraping.linkedin_scraper |     ✓ Parsed: Data Scientist | Seniority: Unknown | Skills: 11 required\n",
      "2026-02-10 20:55:08,980 | INFO    | src.scraping.linkedin_scraper | Phase 2 complete: 25/25 records parsed (100% success rate)\n",
      "2026-02-10 20:55:08,988 | INFO    | src.pipeline | Schema validation: PASSED\n",
      "2026-02-10 20:55:09,012 | INFO    | src.pipeline | Saved Parquet: ../data/processed\\jobs_v0.2_20260210_205508.parquet\n",
      "2026-02-10 20:55:09,031 | INFO    | src.pipeline | Saved CSV: ../data/processed\\jobs_v0.2_20260210_205508.csv\n",
      "2026-02-10 20:55:09,033 | INFO    | src.pipeline | Saved scrape report: outputs/tables\\scrape_report_20260210_205509.json\n",
      "2026-02-10 20:55:09,034 | INFO    | src.pipeline | ============================================================\n",
      "2026-02-10 20:55:09,035 | INFO    | src.pipeline | PIPELINE COMPLETE\n",
      "2026-02-10 20:55:09,035 | INFO    | src.pipeline | Records: 25\n",
      "2026-02-10 20:55:09,036 | INFO    | src.pipeline | Failures: 0\n",
      "2026-02-10 20:55:09,037 | INFO    | src.pipeline | Output: {'parquet': '../data/processed\\\\jobs_v0.2_20260210_205508.parquet', 'parquet_latest': '../data/processed\\\\jobs_latest.parquet', 'csv': '../data/processed\\\\jobs_v0.2_20260210_205508.csv'}\n",
      "2026-02-10 20:55:09,038 | INFO    | src.pipeline | Log: outputs/logs\\scrape_20260210_205310.log\n",
      "2026-02-10 20:55:09,039 | INFO    | src.pipeline | ============================================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "SUCCESS: 25 structured job records produced\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Set working directory to project root for pipeline paths\n",
    "os.chdir(project_root)\n",
    "\n",
    "# Run the full pipeline\n",
    "df = run_pipeline(\n",
    "    keywords=KEYWORDS,\n",
    "    location=LOCATION,\n",
    "    limit=LIMIT,\n",
    "    output_dir=PROCESSED_OUTPUT_DIR,\n",
    "    raw_output_dir=RAW_OUTPUT_DIR,\n",
    "    save_csv=True,\n",
    ")\n",
    "\n",
    "if df is not None:\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"SUCCESS: {len(df)} structured job records produced\")\n",
    "    print(f\"{'='*60}\")\n",
    "else:\n",
    "    print(\"Pipeline returned no data. Check logs in outputs/logs/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f6cf34d",
   "metadata": {},
   "source": [
    "## Inspect Results\n",
    "Quick validation that the data supports downstream analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "082fc25d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "─── Schema Columns ───\n",
      "Columns (25): ['job_id', 'company', 'raw_title', 'normalized_title', 'title_keywords', 'role_type', 'seniority_level', 'skills_required', 'skills_optional', 'tools_frameworks', 'min_years_experience', 'experience_text', 'employment_type', 'work_mode', 'city', 'region', 'country', 'date_posted_raw', 'date_posted_normalized', 'job_description_raw', 'source', 'job_url', 'scrape_timestamp', 'schema_version', 'dataset_version']\n",
      "\n",
      "─── Sample Record ───\n",
      "  job_id                         = 2c6a4738b0788d7f\n",
      "  company                        = Tinder\n",
      "  raw_title                      = Data Scientist Intern\n",
      "  normalized_title               = Data Scientist\n",
      "  title_keywords                 = ['Intern']\n",
      "  role_type                      = GenAI / LLM Engineer\n",
      "  seniority_level                = Senior\n",
      "  skills_required                = ['Large Language Models', 'Machine Learning', 'Python', 'SQL']\n",
      "  skills_optional                = []\n",
      "  tools_frameworks               = ['Python', 'SQL']\n",
      "  min_years_experience           = nan\n",
      "  experience_text                = nan\n",
      "  employment_type                = Internship\n",
      "  work_mode                      = Hybrid\n",
      "  city                           = Los Angeles\n",
      "  region                         = California\n",
      "  country                        = United States\n",
      "  date_posted_raw                = 5 days ago\n",
      "  date_posted_normalized         = 2026-02-05\n",
      "  job_description_raw            = Our Mission\n",
      "As humans, there are few things more exciting than meeting someone new. At Tinder, we’re inspired by the cha...\n",
      "  source                         = linkedin\n",
      "  job_url                        = https://www.linkedin.com/jobs/view/data-scientist-intern-at-tinder-4323761210\n",
      "  scrape_timestamp               = 2026-02-10T20:53:25.085988\n",
      "  schema_version                 = 0.2\n",
      "  dataset_version                = v0.2\n",
      "\n",
      "─── Distribution Checks ───\n",
      "\n",
      "Seniority levels:\n",
      "seniority_level\n",
      "Senior        21\n",
      "Staff/Lead     3\n",
      "Unknown        1\n",
      "\n",
      "Role types:\n",
      "role_type\n",
      "Analytics-focused Data Scientist    16\n",
      "GenAI / LLM Engineer                 4\n",
      "Applied Data Scientist               2\n",
      "Research Scientist                   2\n",
      "Machine Learning Engineer            1\n",
      "\n",
      "Work modes:\n",
      "work_mode\n",
      "Hybrid     11\n",
      "Unknown    11\n",
      "Remote      3\n",
      "\n",
      "Employment types:\n",
      "employment_type\n",
      "Full-time     23\n",
      "Internship     2\n",
      "\n",
      "Top 15 required skills:\n",
      "  SQL                             20 ( 80.0%)\n",
      "  Python                          17 ( 68.0%)\n",
      "  Machine Learning                12 ( 48.0%)\n",
      "  RAG                             12 ( 48.0%)\n",
      "  Rust                             9 ( 36.0%)\n",
      "  A/B Testing                      7 ( 28.0%)\n",
      "  R                                7 ( 28.0%)\n",
      "  Scala                            6 ( 24.0%)\n",
      "  Apache Spark                     5 ( 20.0%)\n",
      "  Tableau                          5 ( 20.0%)\n",
      "  Forecasting                      5 ( 20.0%)\n",
      "  Git                              4 ( 16.0%)\n",
      "  Large Language Models            3 ( 12.0%)\n",
      "  Apache Airflow                   3 ( 12.0%)\n",
      "  Feature Engineering              3 ( 12.0%)\n",
      "\n",
      "─── Final Validation Checks ───\n",
      "  ✓ Can compare Junior vs Senior: True\n",
      "  ✓ Can distinguish role types: True\n",
      "  ✓ Can compute skill frequency: True\n",
      "  ✓ Has raw descriptions: True\n"
     ]
    }
   ],
   "source": [
    "if df is not None and not df.empty:\n",
    "    print(\"─── Schema Columns ───\")\n",
    "    print(f\"Columns ({len(df.columns)}): {list(df.columns)}\\n\")\n",
    "\n",
    "    print(\"─── Sample Record ───\")\n",
    "    sample = df.iloc[0].to_dict()\n",
    "    for k, v in sample.items():\n",
    "        val_repr = repr(v) if isinstance(v, list) else str(v)\n",
    "        if len(val_repr) > 120:\n",
    "            val_repr = val_repr[:120] + \"...\"\n",
    "        print(f\"  {k:30s} = {val_repr}\")\n",
    "\n",
    "    print(\"\\n─── Distribution Checks ───\")\n",
    "    print(f\"\\nSeniority levels:\\n{df['seniority_level'].value_counts().to_string()}\")\n",
    "    print(f\"\\nRole types:\\n{df['role_type'].value_counts().to_string()}\")\n",
    "    print(f\"\\nWork modes:\\n{df['work_mode'].value_counts().to_string()}\")\n",
    "    print(f\"\\nEmployment types:\\n{df['employment_type'].value_counts().to_string()}\")\n",
    "\n",
    "    # Skill frequency (top 15)\n",
    "    from collections import Counter\n",
    "    all_skills = [s for skills in df['skills_required'] for s in skills]\n",
    "    skill_freq = Counter(all_skills).most_common(15)\n",
    "    print(f\"\\nTop 15 required skills:\")\n",
    "    for skill, count in skill_freq:\n",
    "        pct = count / len(df) * 100\n",
    "        print(f\"  {skill:30s} {count:3d} ({pct:5.1f}%)\")\n",
    "\n",
    "    # Verify the 4 final checks\n",
    "    print(\"\\n─── Final Validation Checks ───\")\n",
    "    print(f\"  ✓ Can compare Junior vs Senior: {df['seniority_level'].nunique() > 1}\")\n",
    "    print(f\"  ✓ Can distinguish role types: {df['role_type'].nunique() > 1}\")\n",
    "    print(f\"  ✓ Can compute skill frequency: {len(all_skills) > 0}\")\n",
    "    print(f\"  ✓ Has raw descriptions: {df['job_description_raw'].notna().all()}\")\n",
    "else:\n",
    "    print(\"No data available. Run the pipeline cell above first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d8bfd72",
   "metadata": {},
   "source": [
    "## Skill Co-occurrence & Ideal Candidate Profile\n",
    "Compute which skills appear together most often — the foundation for building an \"ideal candidate profile\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c270c183",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "─── Top 15 Skill Co-occurrences ───\n",
      "  Python                    + SQL                        (14 jobs)\n",
      "  RAG                       + SQL                        (11 jobs)\n",
      "  Machine Learning          + Python                     (10 jobs)\n",
      "  Machine Learning          + SQL                        (9 jobs)\n",
      "  RAG                       + Rust                       (8 jobs)\n",
      "  Rust                      + SQL                        (7 jobs)\n",
      "  R                         + SQL                        (7 jobs)\n",
      "  Python                    + R                          (6 jobs)\n",
      "  Python                    + RAG                        (6 jobs)\n",
      "  A/B Testing               + RAG                        (5 jobs)\n",
      "  A/B Testing               + SQL                        (5 jobs)\n",
      "  Apache Spark              + SQL                        (5 jobs)\n",
      "  RAG                       + Tableau                    (5 jobs)\n",
      "  SQL                       + Tableau                    (5 jobs)\n",
      "  Python                    + Scala                      (5 jobs)\n",
      "\n",
      "─── Skills by Seniority Level ───\n",
      "\n",
      "  Senior (n=21, avg skills=5.3):\n",
      "    SQL: 18\n",
      "    Python: 13\n",
      "    Machine Learning: 10\n",
      "    RAG: 10\n",
      "    Rust: 8\n",
      "\n",
      "  Staff/Lead (n=3, avg skills=5.7):\n",
      "    Python: 3\n",
      "    Scala: 2\n",
      "    A/B Testing: 2\n",
      "    Machine Learning: 2\n",
      "    Git: 1\n",
      "\n",
      "  Unknown (n=1, avg skills=11.0):\n",
      "    Apache Airflow: 1\n",
      "    BigQuery: 1\n",
      "    GCP: 1\n",
      "    Pandas: 1\n",
      "    Python: 1\n",
      "\n",
      "─── Output Files ───\n",
      "  Parquet: data/processed/jobs_latest.parquet\n",
      "  Use: pd.read_parquet('data/processed/jobs_latest.parquet')\n"
     ]
    }
   ],
   "source": [
    "if df is not None and not df.empty:\n",
    "    from itertools import combinations\n",
    "    from collections import Counter\n",
    "\n",
    "    # Skill co-occurrence matrix (top pairs)\n",
    "    pair_counts = Counter()\n",
    "    for skills in df['skills_required']:\n",
    "        for pair in combinations(sorted(set(skills)), 2):\n",
    "            pair_counts[pair] += 1\n",
    "\n",
    "    print(\"─── Top 15 Skill Co-occurrences ───\")\n",
    "    for (s1, s2), count in pair_counts.most_common(15):\n",
    "        print(f\"  {s1:25s} + {s2:25s}  ({count} jobs)\")\n",
    "\n",
    "    # Ideal candidate profile: average skills per seniority\n",
    "    print(\"\\n─── Skills by Seniority Level ───\")\n",
    "    for level in df['seniority_level'].unique():\n",
    "        subset = df[df['seniority_level'] == level]\n",
    "        all_req = [s for skills in subset['skills_required'] for s in skills]\n",
    "        top_5 = Counter(all_req).most_common(5)\n",
    "        avg_skills = subset['skills_required'].apply(len).mean()\n",
    "        print(f\"\\n  {level} (n={len(subset)}, avg skills={avg_skills:.1f}):\")\n",
    "        for skill, cnt in top_5:\n",
    "            print(f\"    {skill}: {cnt}\")\n",
    "\n",
    "    # Save processed dataset path for downstream notebooks\n",
    "    print(f\"\\n─── Output Files ───\")\n",
    "    print(f\"  Parquet: data/processed/jobs_latest.parquet\")\n",
    "    print(f\"  Use: pd.read_parquet('data/processed/jobs_latest.parquet')\")\n",
    "else:\n",
    "    print(\"No data available.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "034f5f3f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.14.0)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
